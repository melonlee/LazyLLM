# 语言建模原理

## 🎯 什么是语言建模？

语言建模是指让计算机学习和理解语言的概率分布过程。它是大语言模型(LLM)的核心任务，通过预测序列中下一个词的概率来实现。

**核心思想**：
- 给定前面的词，预测下一个词出现的概率
- 学习语言的规律、语法和语义
- 建立对人类语言的统计理解

### 语言建模的目标

```
输入: "太阳从东方..."
目标: 预测下一个词
可能输出: "升起"(概率: 0.8), "出现"(概率: 0.15), "照耀"(概率: 0.05)
```

## 🔤 词元化 (Tokenization)

### 什么是词元化？

词元化是将文本分解成更小单位（词元/tokens）的过程，是语言建模的第一步。

### 词元化方法

#### 1. 按空格分割
```python
文本: "开发者喜欢机器学习"
结果: ["开发者", "喜欢", "机器学习"]
```

**优点**: 简单直观
**缺点**: 无法处理复合词和未知词

#### 2. 按字符分割
```python
文本: "Hello"
结果: ["H", "e", "l", "l", "o"]
```

**优点**: 词汇表小，不存在未知词
**缺点**: 序列长，语义信息丢失

#### 3. 子词分割 (Subword Tokenization)
```python
文本: "开发者的最爱"
结果: ["开发", "者", "的", "最", "爱"]
```

**优点**: 平衡词汇表大小和语义保留
**缺点**: 实现复杂

### 字节对编码 (Byte Pair Encoding, BPE)

BPE是现代LLM中最常用的词元化方法：

#### 工作原理
1. **初始化**: 将文本分解为字符
2. **统计**: 计算相邻字符对的频率
3. **合并**: 合并最频繁的字符对
4. **重复**: 重复步骤2-3直到达到目标词汇表大小

#### 示例过程
```
原文: "hello hello world"
初始: ["h", "e", "l", "l", "o", " ", "h", "e", "l", "l", "o", " ", "w", "o", "r", "l", "d"]

第1轮: 合并最频繁的"l"+"l" → "ll"
结果: ["h", "e", "ll", "o", " ", "h", "e", "ll", "o", " ", "w", "o", "r", "l", "d"]

第2轮: 合并"h"+"e" → "he"
结果: ["he", "ll", "o", " ", "he", "ll", "o", " ", "w", "o", "r", "l", "d"]

...继续直到达到目标词汇表大小
```

### 词元化的实际应用

#### GPT系列模型
- 使用BPE方法
- 词汇表大小：约50,000个词元
- 平均每个英文单词对应1.3个词元

#### BERT模型
- 使用WordPiece方法
- 词汇表大小：约30,000个词元
- 支持多语言

## 🌐 词嵌入 (Word Embeddings)

### 什么是词嵌入？

词嵌入是将词元转换为数值向量的过程，使计算机能够理解和处理文本。

### 词嵌入的特点

#### 1. 向量表示
每个词元被表示为一个固定维度的向量：
```
"猫" → [0.2, -0.1, 0.8, 0.3, ..., 0.5]  # 512维向量
"狗" → [0.1, -0.2, 0.7, 0.4, ..., 0.6]  # 512维向量
```

#### 2. 语义相似性
相似含义的词具有相似的向量：
```python
相似度("猫", "狗") = 0.85  # 都是动物
相似度("猫", "汽车") = 0.12  # 差异很大
```

#### 3. 语义运算
词嵌入支持向量运算：
```python
"国王" - "男人" + "女人" ≈ "女王"
```

### 词嵌入的训练

#### 1. 随机初始化
```python
# 模型开始时随机分配向量
"快乐" → [0.123, -0.456, 0.789, ...]  # 随机值
"悲伤" → [0.987, -0.654, 0.321, ...]  # 随机值
```

#### 2. 训练调整
通过大量文本训练，调整向量使其反映真实的语义关系：
```python
# 训练后
"快乐" → [0.8, 0.6, -0.2, ...]   # 积极情感
"悲伤" → [-0.7, -0.5, 0.1, ...]  # 消极情感
```

### 位置编码 (Positional Encoding)

由于Transformer没有循环结构，需要位置编码来表示词序信息。

#### 绝对位置编码
```python
输入: ["我", "喜欢", "编程"]
位置: [0, 1, 2]

最终输入 = 词嵌入 + 位置编码
```

#### 相对位置编码
```python
# 关注词与词之间的相对距离
"我" 与 "喜欢" 的距离: 1
"我" 与 "编程" 的距离: 2
```

## 🎲 语言建模的概率计算

### 条件概率
语言模型计算给定前文的条件下，下一个词的概率：

```
P(w_t | w_1, w_2, ..., w_{t-1})
```

### 链式法则
整个句子的概率是各个词概率的乘积：

```
P(句子) = P(w_1) × P(w_2|w_1) × P(w_3|w_1,w_2) × ... × P(w_n|w_1,...,w_{n-1})
```

### 实际例子
```
句子: "我喜欢编程"
P("我") = 0.05
P("喜欢"|"我") = 0.3
P("编程"|"我","喜欢") = 0.4
P(整个句子) = 0.05 × 0.3 × 0.4 = 0.006
```

## 🔍 预测过程

### 1. 输入处理
```python
输入文本: "今天天气很"
词元化: ["今天", "天气", "很"]
转换ID: [1234, 5678, 9012]
```

### 2. 嵌入转换
```python
词嵌入: [[0.1, 0.2, ...], [0.3, 0.4, ...], [0.5, 0.6, ...]]
位置编码: [[0.0, 0.1, ...], [0.1, 0.2, ...], [0.2, 0.3, ...]]
最终输入: 词嵌入 + 位置编码
```

### 3. 模型计算
```python
# 通过Transformer层处理
输出概率分布: {
    "好": 0.4,
    "晴朗": 0.3,
    "不错": 0.2,
    "糟糕": 0.1
}
```

### 4. 词选择
```python
# 根据概率选择下一个词
选择结果: "好" (概率最高)
```

## 🎯 上下文窗口

### 什么是上下文窗口？

上下文窗口是模型一次能够处理的最大词元数量。

### 窗口大小影响

#### 小窗口 (如1024个词元)
- **优点**: 计算效率高，内存需求小
- **缺点**: 无法处理长文档，理解能力有限

#### 大窗口 (如32768个词元)
- **优点**: 能理解长文档，上下文理解更全面
- **缺点**: 计算资源需求大，训练成本高

### 实际应用
```python
# GPT-3.5
上下文窗口: 4096个词元
约等于: 3000个英文单词

# GPT-4
上下文窗口: 8192个词元
约等于: 6000个英文单词

# Claude-2
上下文窗口: 100000个词元
约等于: 75000个英文单词
```

## 🎓 学习要点

### 核心概念理解
- [ ] 理解语言建模的基本原理
- [ ] 掌握词元化的不同方法
- [ ] 理解词嵌入的作用和特点
- [ ] 了解位置编码的必要性
- [ ] 理解上下文窗口的概念

### 实践建议
1. **动手实验**: 尝试不同的词元化方法
2. **可视化嵌入**: 使用工具查看词嵌入的分布
3. **概率计算**: 理解模型如何计算下一个词的概率
4. **窗口实验**: 测试不同上下文长度对结果的影响

## 📚 进阶学习

### 深入理解
- **数学基础**: 概率论、信息论
- **优化方法**: 如何训练更好的嵌入
- **评估指标**: 困惑度、BLEU等

### 实际应用
- **文本预处理**: 构建高质量的词元化器
- **嵌入优化**: 改进词嵌入质量
- **长文本处理**: 处理超长上下文的技术

---