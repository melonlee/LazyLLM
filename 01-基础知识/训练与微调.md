# 训练与微调

## 🎯 训练流程概述

大语言模型的训练是一个复杂的多阶段过程，每个阶段都有不同的目标和方法。

### 完整训练流程

```
原始数据 → 预训练 → 基础模型 → 监督微调 → 指令模型 → RLHF → 最终模型
```

## 🏗️ 预训练 (Pre-training)

### 什么是预训练？

预训练是在大规模无标签文本数据上训练模型的过程，目标是让模型学会语言的基本规律和知识。

### 预训练的特点

#### 1. 无监督学习
- **数据来源**: 互联网文本、书籍、文章等
- **标签**: 不需要人工标注
- **目标**: 学习语言的统计规律

#### 2. 自监督任务
**下一个词预测**：
```
输入: "今天天气很"
目标: 预测下一个词 "好"
```

### 预训练数据

#### 数据规模
- **GPT-3**: 45TB的文本数据
- **LLaMA**: 1.4TB的文本数据
- **包含**: 网页、书籍、新闻、论坛等

#### 数据质量
- **去重**: 移除重复内容
- **过滤**: 移除低质量文本
- **清洗**: 标准化格式

### 预训练过程

#### 1. 数据准备
```python
# 数据预处理流程
原始文本 → 清洗 → 词元化 → 批处理 → 训练
```

#### 2. 模型初始化
```python
# 随机初始化模型参数
权重矩阵 = 随机生成()
偏置向量 = 零向量()
```

#### 3. 迭代训练
```python
for epoch in range(训练轮数):
    for batch in 数据批次:
        # 前向传播
        预测 = 模型(输入)
        损失 = 计算损失(预测, 真实标签)
        
        # 反向传播
        梯度 = 计算梯度(损失)
        更新参数(梯度)
```

## 🎯 监督微调 (Supervised Fine-tuning, SFT)

### 什么是监督微调？

监督微调是在特定任务的标注数据上进一步训练预训练模型的过程。

### 微调的目标

#### 1. 任务适配
```python
# 文本分类任务
输入: "这部电影很棒"
输出: "正面情感"

# 问答任务
输入: "问题：北京的首都是什么？"
输出: "答案：北京是中国的首都"
```

#### 2. 指令遵循
```python
# 指令格式
输入: "请将以下文本翻译成英文：你好"
输出: "Hello"
```

### 微调数据构建

#### 1. 指令数据格式
```json
{
  "instruction": "请总结以下文章的主要观点",
  "input": "文章内容...",
  "output": "主要观点：..."
}
```

#### 2. 数据来源
- **人工标注**: 雇佣标注员创建高质量数据
- **模型生成**: 使用强模型生成训练数据
- **数据增强**: 通过变换扩充数据集

### 微调技术

#### 1. 全量微调 (Full Fine-tuning)
```python
# 更新所有模型参数
for 参数 in 模型.所有参数():
    参数.requires_grad = True
```

**优点**: 效果最好
**缺点**: 计算资源需求大

#### 2. LoRA (Low-Rank Adaptation)
```python
# 只训练低秩适应器
原始权重 = 冻结
LoRA权重 = 可训练
最终权重 = 原始权重 + LoRA权重
```

**优点**: 参数量少，训练快
**缺点**: 效果可能略低于全量微调

#### 3. 适配器 (Adapters)
```python
# 在模型中插入小型神经网络
Transformer层 + 适配器模块
```

**优点**: 参数效率高
**缺点**: 推理速度稍慢

## 🔄 人类反馈强化学习 (RLHF)

### 什么是RLHF？

RLHF是通过人类反馈来训练模型的方法，使模型输出更符合人类偏好。

### RLHF的三个阶段

#### 阶段1: 收集人类反馈
```python
# 比较两个模型输出
输入: "写一个关于友谊的故事"
输出A: "从前有两个好朋友..."
输出B: "友谊是人生最宝贵的财富..."

人类标注: B > A (B更好)
```

#### 阶段2: 训练奖励模型
```python
# 学习人类偏好
奖励模型(输出A) = 0.3
奖励模型(输出B) = 0.8
```

#### 阶段3: 强化学习优化
```python
# 使用PPO算法优化
for 训练步骤 in 强化学习:
    生成输出 = 模型(输入)
    奖励 = 奖励模型(生成输出)
    更新模型参数(奖励)
```

### RLHF的优势

#### 1. 对齐人类价值
- **安全性**: 减少有害输出
- **有用性**: 提高回答质量
- **诚实性**: 减少幻觉现象

#### 2. 灵活性
- **主观任务**: 创意写作、对话
- **价值观**: 道德、伦理判断
- **偏好**: 风格、语调

## 🎲 训练超参数

### 学习率 (Learning Rate)
```python
# 学习率调度
初始学习率 = 1e-4
学习率衰减 = 余弦退火
最小学习率 = 1e-6
```

### 批次大小 (Batch Size)
```python
# 不同阶段的批次大小
预训练: 2048 序列
微调: 64 序列
RLHF: 32 序列
```

### 训练轮数 (Epochs)
```python
# 避免过拟合
预训练: 1 epoch (数据量大)
微调: 3-5 epochs
RLHF: 1-2 epochs
```

## 📊 评估指标

### 预训练评估
#### 困惑度 (Perplexity)
```python
困惑度 = 2^(交叉熵损失)
# 越低越好，表示模型预测越准确
```

#### 下游任务性能
```python
# 在特定任务上测试
文本分类准确率 = 85%
问答任务F1分数 = 78%
```

### 微调评估
#### 任务相关指标
```python
# 根据具体任务选择
分类: 准确率、F1分数
生成: BLEU、ROUGE分数
对话: 人工评估
```

### RLHF评估
#### 人类评估
```python
# 人类评估标准
有用性: 4.2/5.0
安全性: 4.5/5.0
诚实性: 3.8/5.0
```

## 🛠️ 实践建议

### 数据准备
1. **质量优于数量**: 高质量数据更重要
2. **多样性**: 覆盖不同领域和任务
3. **平衡性**: 避免偏见和不平衡

### 模型选择
1. **基础模型**: 选择合适的预训练模型
2. **模型大小**: 平衡性能和资源需求
3. **架构适配**: 根据任务选择合适架构

### 训练策略
1. **渐进式训练**: 从简单到复杂
2. **正则化**: 防止过拟合
3. **监控**: 实时监控训练过程

### 资源管理
1. **计算资源**: GPU/TPU集群
2. **存储资源**: 大规模数据存储
3. **时间规划**: 合理安排训练时间

## 🚀 训练技巧

### 梯度累积
```python
# 模拟大批次训练
累积步数 = 4
for step in range(累积步数):
    损失 = 模型(数据) / 累积步数
    损失.backward()
optimizer.step()
```

### 混合精度训练
```python
# 使用FP16加速训练
with torch.cuda.amp.autocast():
    输出 = 模型(输入)
    损失 = 损失函数(输出, 标签)
```

### 梯度检查点
```python
# 节省GPU内存
torch.utils.checkpoint.checkpoint(模型层, 输入)
```

## 📚 学习要点

### 核心概念掌握
- [ ] 理解预训练、微调、RLHF的区别
- [ ] 掌握不同训练阶段的目标
- [ ] 了解各种微调技术的特点
- [ ] 理解评估指标的含义

### 实践技能
- [ ] 能够准备训练数据
- [ ] 会设置训练参数
- [ ] 能够监控训练过程
- [ ] 会评估模型效果

## 🎯 进阶学习

### 深入理解
- **优化算法**: Adam、AdamW等
- **分布式训练**: 多GPU、多机训练
- **模型压缩**: 剪枝、量化、蒸馏

### 实际应用
- **领域适应**: 特定领域的模型训练
- **多任务学习**: 同时训练多个任务
- **持续学习**: 增量学习新知识

---

**知识来源**: OpenAI、Anthropic等公司的技术论文